{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWqzYdljAUau"
      },
      "outputs": [],
      "source": [
        "# Binary Class\n",
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "  return x * (1 - x)\n",
        "\n",
        "def mean_squared_error_loss(y_true, y_pred):\n",
        "  return np.mean(np.power(y_true - y_pred, 2))\n",
        "\n",
        "## input\n",
        "input_data = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
        "output_data = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "## Traning Parameter\n",
        "np.random.seed(42)\n",
        "input_size = 2\n",
        "hidden_size = 2\n",
        "output_size = 1\n",
        "\n",
        "## Random Weights & Bias\n",
        "weights_input_to_hidden = np.random.rand(input_size, hidden_size)\n",
        "bias_hidden = np.random.rand(hidden_size)\n",
        "\n",
        "weights_hidden_to_output = np.random.rand(hidden_size, output_size)\n",
        "bias_output = np.random.rand(output_size)\n",
        "\n",
        "#Training Parameter\n",
        "learning_rate = 0.1\n",
        "epochs = 10000\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  #forward pass\n",
        "  hidden_input = np.dot(input_data, weights_input_to_hidden)+bias_hidden\n",
        "  hidden_output = sigmoid(hidden_input)\n",
        "\n",
        "  final_input = np.dot(hidden_output, weights_hidden_to_output)+bias_output\n",
        "  final_output = sigmoid(final_input)\n",
        "\n",
        "\n",
        "  #computing loss\n",
        "  loss = mean_squared_error_loss(output_data, final_output)\n",
        "\n",
        "  #backpropagation\n",
        "  error = final_output-output_data\n",
        "  gradient_output = error*sigmoid_derivative(final_output)\n",
        "\n",
        "  # hidden layer eror and gradient\n",
        "  error_hidden = gradient_output.dot(weights_hidden_to_output.T)\n",
        "  gradient_hidden = error_hidden*sigmoid_derivative(hidden_output)\n",
        "\n",
        "  #updatingweight and biases\n",
        "  weights_hidden_to_output-=learning_rate*np.dot(hidden_output.T, gradient_output)\n",
        "  bias_output-=learning_rate*np.mean(gradient_output, axis=0)\n",
        "\n",
        "  weights_input_to_hidden-=learning_rate*np.dot(input_data.T, gradient_hidden)\n",
        "  bias_hidden-=learning_rate*np.mean(gradient_hidden, axis=0)\n",
        "\n",
        "  #print 1000 loss\n",
        "  if epoch%1000==0:\n",
        "    print(f'Epoch: {epoch}, Loss: {loss}')\n",
        "\n",
        "#compute out for each input pair after training\n",
        "result = []\n",
        "for input_pair in input_data:\n",
        "  hidden_input = np.dot(input_pair, weights_input_to_hidden)+bias_hidden\n",
        "  hidden_output = sigmoid(hidden_input)\n",
        "  final_input = np.dot(hidden_output, weights_hidden_to_output)+bias_output\n",
        "  final_output = sigmoid(final_input)\n",
        "  result.append(final_output)\n",
        "  print(f'Input: {input_pair}, Output: {final_output}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## MultiClass\n",
        "import numpy as np\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "def softmax(x):\n",
        "    exps = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "    return exps / np.sum(exps, axis=-1, keepdims=True)\n",
        "\n",
        "def categorical_crossentropy_loss(y_true, y_pred):\n",
        "    return -np.sum(y_true * np.log(y_pred))\n",
        "\n",
        "# Input data and output data\n",
        "input_data = np.array([[0.8, 0.6, 0.7]])\n",
        "output_data = np.array([[0, 1, 0]])\n",
        "\n",
        "# Initialize weights and biases\n",
        "weights_input_to_hidden = np.array([[0.2, 0.4, 0.1],\n",
        "                                    [0.5, 0.3, 0.2],\n",
        "                                    [0.3, 0.7, 0.8]])\n",
        "\n",
        "bias_hidden = np.array([0.1, 0.2, 0.3])\n",
        "\n",
        "weights_hidden_to_output = np.array([[0.6, 0.4, 0.5],\n",
        "                                     [0.1, 0.2, 0.3],\n",
        "                                     [0.3, 0.7, 0.2]])\n",
        "\n",
        "bias_output = np.array([0.1, 0.2, 0.3])\n",
        "\n",
        "# Learning rate\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Training loop\n",
        "epochs = 1000\n",
        "for epoch in range(epochs):\n",
        "    # Forward Pass\n",
        "    hidden_input = np.dot(input_data, weights_input_to_hidden) + bias_hidden\n",
        "    hidden_output = relu(hidden_input)\n",
        "\n",
        "    final_input = np.dot(hidden_output, weights_hidden_to_output) + bias_output\n",
        "    final_output = softmax(final_input)\n",
        "\n",
        "    # Compute loss\n",
        "    loss = categorical_crossentropy_loss(output_data, final_output)\n",
        "\n",
        "    # Print loss every 100 epochs\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}: Loss {loss:.4f}\")\n",
        "\n",
        "    # Backpropagation\n",
        "    error = final_output - output_data\n",
        "\n",
        "    # Gradient for weights_hidden_to_output and bias_output\n",
        "    gradient_weights_hidden_to_output = np.dot(hidden_output.T, error)\n",
        "    gradient_bias_output = np.sum(error, axis=0, keepdims=True)\n",
        "\n",
        "    # Propagate the error back to the hidden layer\n",
        "    error_hidden = np.dot(error, weights_hidden_to_output.T)\n",
        "    error_hidden *= relu_derivative(hidden_input)\n",
        "\n",
        "    # Gradient for weights_input_to_hidden and bias_hidden\n",
        "    gradient_weights_input_to_hidden = np.dot(input_data.T, error_hidden)\n",
        "    gradient_bias_hidden = np.sum(error_hidden, axis=0, keepdims=True)\n",
        "\n",
        "    # Update weights and biases using gradient descent\n",
        "    weights_hidden_to_output -= learning_rate * gradient_weights_hidden_to_output\n",
        "    bias_output -= learning_rate * gradient_bias_output.squeeze()\n",
        "    weights_input_to_hidden -= learning_rate * gradient_weights_input_to_hidden\n",
        "    bias_hidden -= learning_rate * gradient_bias_hidden.squeeze()\n",
        "\n",
        "# Print final loss after training\n",
        "print(f\"Final Loss: {loss:.4f}\")\n",
        "print(f\"Input: {input_data}, Output : {final_output}\")"
      ],
      "metadata": {
        "id": "Uq-6_Cmk9MBN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}